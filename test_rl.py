#!/usr/bin/env python3
"""Test simple du systÃ¨me RL"""

import numpy as np
import trimesh
from coverage.mesh_rl_env import MeshCoverageRLEnv
from coverage.dqn_agent import DQNAgent
from coverage.mesh_generator import ProceduralMeshGenerator

print("ğŸ”§ Initialisation...")

# GÃ©nÃ©rer un mesh simple
mesh = ProceduralMeshGenerator.simple_sphere(subdivisions=2)

print(f"ğŸ“Š Mesh: {len(mesh.vertices)} vertices, {len(mesh.faces)} faces")

# CrÃ©er l'env
env = MeshCoverageRLEnv(mesh, coverage_radius=0.15, max_steps=500)

# Test reset
state = env.reset()
print(f"ğŸ® Ã‰tat dim: {len(state)}")
print(f"ğŸ“ Action space: 8 directions discrÃ¨tes")

# CrÃ©er l'agent
agent = DQNAgent(state_dim=len(state), action_dim=8)

print("âœ… SystÃ¨me RL prÃªt!")
print("\nğŸš€ Test d'un pas:")

# Test step
action = 0  # direction 0
state_next, reward, done, info = env.step(action)
print(f"   RÃ©compense: {reward:.3f}")
print(f"   Couverture: {(env.coverage_map.sum() / env.n_faces)*100:.1f}%")
print(f"   Ã‰tapes: {env.step_count}/{env.max_steps}")

print("\nâœ¨ Tous les composants fonctionnent!")
print("   Pour un entraÃ®nement complet: python coverage/rl_trainer.py")
